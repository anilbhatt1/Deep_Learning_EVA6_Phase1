{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session 13 ViT Baby Steps.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NX257Zn42LmY"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FNvWXk53KTG",
        "outputId": "122eb8e3-d3f4-4f8b-c27d-d529195cb8b6"
      },
      "source": [
        "import collections.abc\n",
        "\n",
        "def to_2tuple(x):\n",
        "  if isinstance(x, collections.abc.Iterable):\n",
        "    return x\n",
        "  return x, x\n",
        "\n",
        "img_size = 224\n",
        "patch_size = 16\n",
        "\n",
        "img_size = to_2tuple(img_size)\n",
        "patch_size = to_2tuple(patch_size)\n",
        "\n",
        "img_size, patch_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((224, 224), (16, 16))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYd-PF1B5Byz",
        "outputId": "6a736b2b-652f-42a3-80cd-2ddb81815d88"
      },
      "source": [
        "x = torch.rand((32, 3, 224, 224))\n",
        "x.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 3, 224, 224])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfeoRQ5T6pBy",
        "outputId": "9b5bf05a-9908-4349-a9b1-be8bdb487d3f"
      },
      "source": [
        "proj = nn.Conv2d(3, 768, 16, 16)\n",
        "y = proj(x)\n",
        "y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 768, 14, 14])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AMpp4VL73PS",
        "outputId": "351a2d9d-ea50-4325-fc90-74010ab9d593"
      },
      "source": [
        "y.flatten(2).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 768, 196])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Umay9IpS79tv",
        "outputId": "b3d5f9c3-d370-48c9-a5f1-7afa4042f6c4"
      },
      "source": [
        "y.flatten(2).transpose(1, 2).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 196, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9ZsdYJ74d-f"
      },
      "source": [
        "class PatchEmbeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Image to Patch Embedding.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, image_size=224, patch_size=16, num_channels=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        image_size = to_2tuple(image_size)\n",
        "        patch_size = to_2tuple(patch_size)\n",
        "        num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n",
        "        self.image_size = image_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.projection = nn.Conv2d(num_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, pixel_values):\n",
        "        batch_size, num_channels, height, width = pixel_values.shape\n",
        "        # FIXME look at relaxing size constraints\n",
        "        if height != self.image_size[0] or width != self.image_size[1]:\n",
        "            raise ValueError(\n",
        "                f\"Input image size ({height}*{width}) doesn't match model ({self.image_size[0]}*{self.image_size[1]}).\"\n",
        "            )\n",
        "        x = self.projection(pixel_values).flatten(2).transpose(1, 2)\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuK1plHM8KpI",
        "outputId": "1ab7761e-9e39-4c19-cdb8-a9c0d6a628d0"
      },
      "source": [
        "cls_token = nn.Parameter(torch.zeros(1, 1, 768))\n",
        "cls_token.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjCnC_Ff8eGj",
        "outputId": "e55f7500-1c92-49dd-e985-20cd8e5d63c9"
      },
      "source": [
        "cls_token.expand(32, -1, -1).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 1, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwJBrgMT9oxw",
        "outputId": "716c8322-23b4-4951-d692-313083d0c6f0"
      },
      "source": [
        "position_embedding = nn.Parameter(torch.zeros(1, 14*14 + 1, 768))\n",
        "position_embedding.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 197, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1gDi46e8klf"
      },
      "source": [
        "class ViTEmbeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Construct the CLS token, position and patch embeddings.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n",
        "        self.patch_embeddings = PatchEmbeddings(\n",
        "            image_size=config.image_size,\n",
        "            patch_size=config.patch_size,\n",
        "            num_channels=config.num_channels,\n",
        "            embed_dim=config.hidden_size,\n",
        "        )\n",
        "        num_patches = self.patch_embeddings.num_patches\n",
        "        self.position_embeddings = nn.Parameter(torch.zeros(1, num_patches + 1, config.hidden_size))\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, pixel_values):\n",
        "        batch_size = pixel_values.shape[0]\n",
        "        embeddings = self.patch_embeddings(pixel_values)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
        "        embeddings = torch.cat((cls_tokens, embeddings), dim=1)\n",
        "        embeddings = embeddings + self.position_embeddings\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzJlCetE_Kis"
      },
      "source": [
        "class ViTConfig():\n",
        "  def __init__(\n",
        "        self,\n",
        "        hidden_size=768,\n",
        "        num_hidden_layers=12,\n",
        "        num_attention_heads=12,\n",
        "        intermediate_size=3072,\n",
        "        hidden_act=\"gelu\",\n",
        "        hidden_dropout_prob=0.0,\n",
        "        attention_probs_dropout_prob=0.0,\n",
        "        initializer_range=0.02,\n",
        "        layer_norm_eps=1e-12,\n",
        "        is_encoder_decoder=False,\n",
        "        image_size=224,\n",
        "        patch_size=16,\n",
        "        num_channels=3,\n",
        "        **kwargs\n",
        "    ):\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.intermediate_size = intermediate_size\n",
        "        self.hidden_act = hidden_act\n",
        "        self.hidden_dropout_prob = hidden_dropout_prob\n",
        "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
        "        self.initializer_range = initializer_range\n",
        "        self.layer_norm_eps = layer_norm_eps\n",
        "\n",
        "        self.image_size = image_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_channels = num_channels\n",
        "\n",
        "\n",
        "configuration = ViTConfig()\n",
        "# You can read full configuration file here: https://github.com/huggingface/transformers/blob/master/src/transformers/models/vit/configuration_vit.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHiDE5P9A1g-",
        "outputId": "b48b2a08-bd96-4957-bac1-3015f96891e5"
      },
      "source": [
        "vars(configuration)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attention_probs_dropout_prob': 0.0,\n",
              " 'hidden_act': 'gelu',\n",
              " 'hidden_dropout_prob': 0.0,\n",
              " 'hidden_size': 768,\n",
              " 'image_size': 224,\n",
              " 'initializer_range': 0.02,\n",
              " 'intermediate_size': 3072,\n",
              " 'layer_norm_eps': 1e-12,\n",
              " 'num_attention_heads': 12,\n",
              " 'num_channels': 3,\n",
              " 'num_hidden_layers': 12,\n",
              " 'patch_size': 16}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsnUV3xo_Oyd",
        "outputId": "d3c7dc19-9185-4aea-e53e-1d0365d6f554"
      },
      "source": [
        "x = torch.rand((32, 3, 224, 224))\n",
        "vit_emb = ViTEmbeddings(configuration)\n",
        "vit_emb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViTEmbeddings(\n",
              "  (patch_embeddings): PatchEmbeddings(\n",
              "    (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "  )\n",
              "  (dropout): Dropout(p=0.0, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hl9sJWIsAPPL",
        "outputId": "7518be24-98b6-43e7-97d7-0f7377553ff9"
      },
      "source": [
        "out = vit_emb(x)\n",
        "out.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 197, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fW8iYx0KGn7g",
        "outputId": "a82ed1f4-996f-47ba-cd38-bde5eecc8a90"
      },
      "source": [
        "mat = nn.Linear(768, 12*64)\n",
        "mat = mat(out)\n",
        "mat.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 197, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0eUMiLjHbtt",
        "outputId": "b3fec487-5469-4489-a2f4-0b64fe1fd95e"
      },
      "source": [
        "mat.size()[:-1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 197])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sc8OCNbFHwDf",
        "outputId": "a33f2cc2-c961-42c9-c155-93079960dbaf"
      },
      "source": [
        "new_shape = mat.size()[:-1] + (12, 64)\n",
        "new_shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 197, 12, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppYdQnxiIAIx",
        "outputId": "8146aab2-d3cb-4b72-eb59-eb83363e15ce"
      },
      "source": [
        "print(out.shape)\n",
        "out = out.view(*new_shape)\n",
        "print(out.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([32, 197, 768])\n",
            "torch.Size([32, 197, 12, 64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cvl4O7KIGHx",
        "outputId": "8c4e0392-0c8c-457e-e040-83ff507789a5"
      },
      "source": [
        "out = out.permute(0, 2, 1, 3)\n",
        "print(out.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([32, 12, 197, 64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4QiBfo1IZbb"
      },
      "source": [
        "out2 = out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "id": "qnOhmjSTIdGQ",
        "outputId": "30809aa3-56da-406e-8a2f-68688b3c5568"
      },
      "source": [
        "torch.matmul(out, out2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-db20b78eb9ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected batch2_sizes[0] == bs && batch2_sizes[1] == contraction_size to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXt_o84hIpLM",
        "outputId": "4211c544-a890-4507-c218-95fe25a2e38d"
      },
      "source": [
        "out2.transpose(-1, -2).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 12, 64, 197])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Z6OpiPXIhVf",
        "outputId": "2c03ef38-ad2e-4dba-db42-38e9c44e5e62"
      },
      "source": [
        "attention_scores = torch.matmul(out, out2.transpose(-1, -2))\n",
        "attention_scores.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 12, 197, 197])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7SVoqQaLHNJ",
        "outputId": "6387059c-8745-4634-f80c-269cbd0f1e10"
      },
      "source": [
        "context_layer = torch.matmul(nn.Softmax(dim=-1)(attention_scores), out)\n",
        "context_layer.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 12, 197, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1RRqMd0MC7m",
        "outputId": "027b4f1d-aa21-4a1b-c36d-d7c890af2192"
      },
      "source": [
        "context_layer = context_layer.permute(0, 2, 1, 3)\n",
        "context_layer.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 197, 12, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktk9akuvLi82",
        "outputId": "4fc15e1f-5563-49a6-cd4d-2d153f02c9f3"
      },
      "source": [
        "context_layer.size()[:-2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 197])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APv2Kd0ILm9f",
        "outputId": "b620cf7c-6e2a-462c-ee15-32d1c6486769"
      },
      "source": [
        "new_context_layer_shape = context_layer.size()[:-2] + (12*64,)\n",
        "new_context_layer_shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 197, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "sD5NCBGNLyOZ",
        "outputId": "be32367b-b61d-45ab-e3e6-aad6c36e43f3"
      },
      "source": [
        "print(context_layer.shape)\n",
        "context_layer.view(*new_context_layer_shape)\n",
        "print(context_layer.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([32, 197, 12, 64])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-67dc3e434ff2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_context_layer_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kd4S9PF8MOHW",
        "outputId": "98000a39-a0c3-4c88-811d-902a7cb20423"
      },
      "source": [
        "context_layer = torch.matmul(nn.Softmax(dim=-1)(attention_scores), out)\n",
        "print(context_layer.shape)\n",
        "context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "print(context_layer.shape)\n",
        "print(context_layer.size()[:-2])\n",
        "new_context_layer_shape = context_layer.size()[:-2] + (12*64,)\n",
        "print(new_context_layer_shape)\n",
        "print(context_layer.shape)\n",
        "context_layer.view(*new_context_layer_shape)\n",
        "print(context_layer.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([32, 12, 197, 64])\n",
            "torch.Size([32, 197, 12, 64])\n",
            "torch.Size([32, 197])\n",
            "torch.Size([32, 197, 768])\n",
            "torch.Size([32, 197, 12, 64])\n",
            "torch.Size([32, 197, 12, 64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9h7y0io9J34R",
        "outputId": "5405cc63-7d26-4274-d69f-53bbd39fe58f"
      },
      "source": [
        "x = torch.randn(3, 2)\n",
        "y = torch.transpose(x, 0, 1)\n",
        "print(x)\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.1447,  0.3310],\n",
            "        [ 0.9052, -0.9171],\n",
            "        [-1.2232, -0.5425]])\n",
            "tensor([[-0.1447,  0.9052, -1.2232],\n",
            "        [ 0.3310, -0.9171, -0.5425]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjEn7xGfKVFM",
        "outputId": "e1fdd0bb-d3eb-45da-ccf8-7cc29174c09b"
      },
      "source": [
        "x[0, 1] = 42\n",
        "print(x)\n",
        "print(y)\n",
        "print(x.is_contiguous())\n",
        "print(y.is_contiguous())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.1447, 42.0000],\n",
            "        [ 0.9052, -0.9171],\n",
            "        [-1.2232, -0.5425]])\n",
            "tensor([[-0.1447,  0.9052, -1.2232],\n",
            "        [42.0000, -0.9171, -0.5425]])\n",
            "True\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnS6oEyMKdsJ"
      },
      "source": [
        "This is where the concept of contiguous comes in. In the example above, x is contiguous but y is not because its memory layout is different to that of a tensor of same shape made from scratch. Note that the word \"contiguous\" is a bit misleading because it's not that the content of the tensor is spread out around disconnected blocks of memory. Here bytes are still allocated in one block of memory but the order of the elements is different!\n",
        "\n",
        "When you call contiguous(), it actually makes a copy of the tensor such that the order of its elements in memory is the same as if it had been created from scratch with the same data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfwBHDYSB4tP"
      },
      "source": [
        "import math\n",
        "class ViTSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
        "            raise ValueError(\n",
        "                f\"The hidden size {config.hidden_size,} is not a multiple of the number of attention \"\n",
        "                f\"heads {config.num_attention_heads}.\"\n",
        "            )\n",
        "\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states, head_mask=None, output_attentions=False):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "\n",
        "        key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "        value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        # Mask heads if we want to\n",
        "        if head_mask is not None:\n",
        "            attention_probs = attention_probs * head_mask\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "\n",
        "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
        "\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rSjTSugaPLf",
        "outputId": "1f5192f2-38bc-4086-f7ed-319de1303d5d"
      },
      "source": [
        "vit_atn = ViTSelfAttention(configuration)\n",
        "vit_atn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViTSelfAttention(\n",
              "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (dropout): Dropout(p=0.0, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "js4KrY1vaXCV",
        "outputId": "f5e0e08d-7027-4f6b-a25a-72c4e425fb00"
      },
      "source": [
        "x = torch.rand((32, 3, 224, 224))\n",
        "vit_emb = ViTEmbeddings(configuration)\n",
        "vit_emb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViTEmbeddings(\n",
              "  (patch_embeddings): PatchEmbeddings(\n",
              "    (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "  )\n",
              "  (dropout): Dropout(p=0.0, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_nQW6Hhalhv",
        "outputId": "9577a623-2ba2-4243-a17f-e2a492ba5980"
      },
      "source": [
        "emb = vit_emb(x)\n",
        "emb.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 197, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4f8vEZLcK_u"
      },
      "source": [
        "context_layer, attention_probs = vit_atn(emb, head_mask=None, output_attentions=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d9ZRsDQcuVg",
        "outputId": "94cf8b61-f6ca-44e7-fda0-2f6294d2aad3"
      },
      "source": [
        "context_layer.shape, attention_probs.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 197, 768]), torch.Size([32, 12, 197, 197]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OZjdVSid142"
      },
      "source": [
        "class ViTSelfOutput(nn.Module):\n",
        "  \"\"\"\n",
        "  This is just a Linear Layer Block\n",
        "  \"\"\"\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "  def forward(self, hidden_states, input_tensor):\n",
        "    hidden_states = self.dense(hidden_states)\n",
        "    hidden_states = self.dropout(hidden_states)\n",
        "\n",
        "    return hidden_states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAstRSHTemTh"
      },
      "source": [
        "class ViTAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attention = ViTSelfAttention(config)\n",
        "        self.output = ViTSelfOutput(config)\n",
        "        self.pruned_heads = set()\n",
        "\n",
        "    def prune_heads(self, heads):\n",
        "        if len(heads) == 0:\n",
        "            return\n",
        "        heads, index = find_pruneable_heads_and_indices(\n",
        "            heads, self.attention.num_attention_heads, self.attention.attention_head_size, self.pruned_heads\n",
        "        )\n",
        "\n",
        "        # Prune linear layers\n",
        "        self.attention.query = prune_linear_layer(self.attention.query, index)\n",
        "        self.attention.key = prune_linear_layer(self.attention.key, index)\n",
        "        self.attention.value = prune_linear_layer(self.attention.value, index)\n",
        "        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n",
        "\n",
        "        # Update hyper params and store pruned heads\n",
        "        self.attention.num_attention_heads = self.attention.num_attention_heads - len(heads)\n",
        "        self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n",
        "        self.pruned_heads = self.pruned_heads.union(heads)\n",
        "\n",
        "    def forward(self, hidden_states, head_mask=None, output_attentions=False):\n",
        "        self_outputs = self.attention(hidden_states, head_mask, output_attentions)\n",
        "\n",
        "        attention_output = self.output(self_outputs[0], hidden_states)\n",
        "\n",
        "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YB6wKnvwjdjB"
      },
      "source": [
        "class ViTIntermediate(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = nn.functional.gelu(hidden_states)\n",
        "\n",
        "        return hidden_states\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xnnRgJDjmqg"
      },
      "source": [
        "class ViTOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "\n",
        "        hidden_states = hidden_states + input_tensor\n",
        "\n",
        "        return hidden_states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7j3Zdvffjx_A"
      },
      "source": [
        "class ViTLayer(nn.Module):\n",
        "    \"\"\"This corresponds to the Block class in the timm implementation.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.seq_len_dim = 1\n",
        "        self.attention = ViTAttention(config)\n",
        "        self.intermediate = ViTIntermediate(config)\n",
        "        self.output = ViTOutput(config)\n",
        "        self.layernorm_before = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.layernorm_after = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "\n",
        "    def forward(self, hidden_states, head_mask=None, output_attentions=False):\n",
        "        self_attention_outputs = self.attention(\n",
        "            self.layernorm_before(hidden_states),  # in ViT, layernorm is applied before self-attention\n",
        "            head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "        )\n",
        "        attention_output = self_attention_outputs[0]\n",
        "        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
        "\n",
        "        # first residual connection\n",
        "        hidden_states = attention_output + hidden_states\n",
        "\n",
        "        # in ViT, layernorm is also applied after self-attention\n",
        "        layer_output = self.layernorm_after(hidden_states)\n",
        "\n",
        "        layer_output = self.intermediate(layer_output)\n",
        "\n",
        "        # second residual connection is done here\n",
        "        layer_output = self.output(layer_output, hidden_states)\n",
        "\n",
        "        outputs = (layer_output,) + outputs\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def feed_forward_chunk(self, attention_output):\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.output(intermediate_output)\n",
        "        return layer_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttuDIk1Sj0bE"
      },
      "source": [
        "class ViTEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layer = nn.ModuleList([ViTLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        head_mask=None,\n",
        "        output_attentions=False,\n",
        "        output_hidden_states=False,\n",
        "        return_dict=True,\n",
        "    ):\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_self_attentions = () if output_attentions else None\n",
        "\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
        "\n",
        "            if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n",
        "\n",
        "                def create_custom_forward(module):\n",
        "                    def custom_forward(*inputs):\n",
        "                        return module(*inputs, output_attentions)\n",
        "\n",
        "                    return custom_forward\n",
        "\n",
        "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
        "                    create_custom_forward(layer_module),\n",
        "                    hidden_states,\n",
        "                    layer_head_mask,\n",
        "                )\n",
        "            else:\n",
        "                layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)\n",
        "\n",
        "            hidden_states = layer_outputs[0]\n",
        "\n",
        "            if output_attentions:\n",
        "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
        "\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "        if not return_dict:\n",
        "            return tuple(v for v in [hidden_states, all_hidden_states, all_self_attentions] if v is not None)\n",
        "        \n",
        "        return hidden_states,all_hidden_states,all_self_attentions\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7PdT_IYj2bd"
      },
      "source": [
        "class ViTModel():\n",
        "    def __init__(self, config, add_pooling_layer=True):\n",
        "        super().__init__(config)\n",
        "        self.config = config\n",
        "\n",
        "        self.embeddings = ViTEmbeddings(config)\n",
        "        self.encoder = ViTEncoder(config)\n",
        "\n",
        "        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.pooler = ViTPooler(config) if add_pooling_layer else None\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embeddings.patch_embeddings\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        pixel_values=None,\n",
        "        head_mask=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        Returns:\n",
        "\n",
        "        Examples::\n",
        "\n",
        "            >>> from transformers import ViTFeatureExtractor, ViTModel\n",
        "            >>> from PIL import Image\n",
        "            >>> import requests\n",
        "\n",
        "            >>> url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
        "            >>> image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "            >>> feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "            >>> model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "\n",
        "            >>> inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
        "            >>> outputs = model(**inputs)\n",
        "            >>> last_hidden_states = outputs.last_hidden_state\n",
        "        \"\"\"\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        if pixel_values is None:\n",
        "            raise ValueError(\"You have to specify pixel_values\")\n",
        "\n",
        "        # Prepare head mask if needed\n",
        "        # 1.0 in head_mask indicate we keep the head\n",
        "        # attention_probs has shape bsz x n_heads x N x N\n",
        "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
        "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
        "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
        "\n",
        "        embedding_output = self.embeddings(pixel_values)\n",
        "\n",
        "        encoder_outputs = self.encoder(\n",
        "            embedding_output,\n",
        "            head_mask=head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        sequence_output = encoder_outputs[0]\n",
        "        sequence_output = self.layernorm(sequence_output)\n",
        "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
        "\n",
        "        if not return_dict:\n",
        "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
        "\n",
        "        return sequence_output,pooled_output,encoder_outputs.hidden_states,encoder_outputs.attentions\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9NcKIiTj8Np"
      },
      "source": [
        "class ViTPooler(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token.\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "        pooled_output = self.activation(pooled_output)\n",
        "        return pooled_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_g0XUxxQmetL",
        "outputId": "674b06b4-4b20-42a8-cae4-a5a3022c7412"
      },
      "source": [
        "out.shape, configuration"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 12, 197, 64]), <__main__.ViTConfig at 0x7f31833ef090>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8faOSm12mpM0"
      },
      "source": [
        "vit_enc = ViTEncoder(configuration)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zWCVuI5naV2",
        "outputId": "8f2e5d01-82f3-46df-b920-e18bc13f102f"
      },
      "source": [
        "vit_enc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViTEncoder(\n",
              "  (layer): ModuleList(\n",
              "    (0): ViTLayer(\n",
              "      (attention): ViTAttention(\n",
              "        (attention): ViTSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (output): ViTSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): ViTIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): ViTOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (1): ViTLayer(\n",
              "      (attention): ViTAttention(\n",
              "        (attention): ViTSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (output): ViTSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): ViTIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): ViTOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (2): ViTLayer(\n",
              "      (attention): ViTAttention(\n",
              "        (attention): ViTSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (output): ViTSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): ViTIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): ViTOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (3): ViTLayer(\n",
              "      (attention): ViTAttention(\n",
              "        (attention): ViTSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (output): ViTSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): ViTIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): ViTOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (4): ViTLayer(\n",
              "      (attention): ViTAttention(\n",
              "        (attention): ViTSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (output): ViTSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): ViTIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): ViTOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (5): ViTLayer(\n",
              "      (attention): ViTAttention(\n",
              "        (attention): ViTSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (output): ViTSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): ViTIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): ViTOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (6): ViTLayer(\n",
              "      (attention): ViTAttention(\n",
              "        (attention): ViTSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (output): ViTSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): ViTIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): ViTOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (7): ViTLayer(\n",
              "      (attention): ViTAttention(\n",
              "        (attention): ViTSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (output): ViTSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): ViTIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): ViTOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (8): ViTLayer(\n",
              "      (attention): ViTAttention(\n",
              "        (attention): ViTSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (output): ViTSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): ViTIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): ViTOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (9): ViTLayer(\n",
              "      (attention): ViTAttention(\n",
              "        (attention): ViTSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (output): ViTSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): ViTIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): ViTOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (10): ViTLayer(\n",
              "      (attention): ViTAttention(\n",
              "        (attention): ViTSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (output): ViTSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): ViTIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): ViTOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (11): ViTLayer(\n",
              "      (attention): ViTAttention(\n",
              "        (attention): ViTSelfAttention(\n",
              "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (output): ViTSelfOutput(\n",
              "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): ViTIntermediate(\n",
              "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "      )\n",
              "      (output): ViTOutput(\n",
              "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "      (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 221
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfmiOnaPoZP4"
      },
      "source": [
        "input = torch.rand((32, 3, 224, 224))\n",
        "embeddings = ViTEmbeddings(configuration)\n",
        "encoder = ViTEncoder(configuration)\n",
        "layernorm = nn.LayerNorm(config.hidden_size, eps=0.000001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PunoKfkkojfC",
        "outputId": "24cd45f8-8101-4aa6-8eb6-c16db886a16c"
      },
      "source": [
        "embedding_output = embeddings(input)\n",
        "embedding_output.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 197, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 223
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvO2dgwapqfO"
      },
      "source": [
        "encoder_output = encoder(embedding_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWv1VqbAqq5G",
        "outputId": "005ae441-2a41-459d-f56e-fc5e081d1e46"
      },
      "source": [
        "type(encoder_output), len(encoder_output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tuple, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 228
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xkw6DnLerCzt",
        "outputId": "c90188f7-d824-4db4-c5eb-035605c7762a"
      },
      "source": [
        "hidden_states, all_hidden_states, all_self_attentions = encoder_output\n",
        "\n",
        "hidden_states.shape, all_hidden_states, all_self_attentions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 197, 768]), None, None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 233
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pppIIEBjtxTL",
        "outputId": "dbdfbef0-b66b-42da-9ea8-807feab113f3"
      },
      "source": [
        "sequence_output = encoder_output[0]\n",
        "sequence_output.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 197, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 234
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHdmbcwXt1U0",
        "outputId": "baa9a2db-50ab-4749-f8a1-f34d1675df4e"
      },
      "source": [
        "sequence_output[:, 0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 235
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Fe0XTYis6Ko",
        "outputId": "bef063ee-4da0-4ed5-da09-334ca16a5233"
      },
      "source": [
        "sequence_output = encoder_output[0]\n",
        "layernorm = nn.LayerNorm(config.hidden_size, eps=0.00001)\n",
        "sequence_output = layernorm(sequence_output)\n",
        "# VitPooler\n",
        "dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "activation = nn.Tanh()\n",
        "first_token_tensor = sequence_output[:, 0]\n",
        "pooled_output = dense(first_token_tensor)\n",
        "pooled_output = activation(pooled_output)\n",
        "pooled_output.shape\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 236
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGb9NtJQuJAf",
        "outputId": "ce2fe8ee-b60f-42bf-c1c4-9d0cae10ee74"
      },
      "source": [
        "classifier = nn.Linear(config.hidden_size, 100)\n",
        "logits = classifier(pooled_output)\n",
        "logits.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 237
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-f5h7cxujpf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}